{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn import svm\n",
    "import time\n",
    "# read in dataset\n",
    "train_path = \"train.txt\"\n",
    "test_path = \"test.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    classes = []\n",
    "    #samples = []\n",
    "    docs = []\n",
    "        \n",
    "    for line in lines:\n",
    "        classes.append(int(line.rsplit()[-1]))\n",
    "        #samples.append(line.rsplit()[0])\n",
    "        #raw = line.decode('latin1')\n",
    "        raw = ' '.join(line.rsplit()[1:-1])\n",
    "        docs.append(raw)\n",
    "    \n",
    "    return (docs, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 192, 102)\n",
      "(\"'\", 197, 279)\n",
      "('?', 2, 14)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = load_data(train_path)\n",
    "X_test, Y_test = load_data(test_path)\n",
    "\n",
    "def search(sequence):\n",
    "    result = []\n",
    "    for word in sequence:   \n",
    "        counter = [1 if word in x else 0 for x in X_train]\n",
    "        indexes = []\n",
    "        for i in range(len(counter)):\n",
    "            if counter[i] != 0:\n",
    "                indexes.append(i)\n",
    "        positive = 0\n",
    "        negative = 0\n",
    "        for i in range(len(indexes)):\n",
    "            if Y_train[indexes[i]] == 1:\n",
    "                positive += 1\n",
    "            else:\n",
    "                negative += 1\n",
    "        result.append((word,positive,negative))\n",
    "    return result\n",
    "\n",
    "chars = ['{','}','#','%','&','\\(','\\)','\\[','\\]','<','>',',', '!', '.', ';', \n",
    "'?', '*', '\\\\', '\\/', '~', '_','|','=','+','^',':','\\\"','\\'','@','-']\n",
    "for element in search([\"!\", \"'\", \"?\"]):\n",
    "    print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "porter = nltk.PorterStemmer() # also lancaster stemmer\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "stopWords = stopwords.words(\"english\")\n",
    "chars = ['{','}','#','%','&','\\(','\\)','\\[','\\]','<','>',',', '!', '.', ';', \n",
    "'?', '*', '\\\\', '\\/', '~', '_','|','=','+','^',':','\\\"','\\'','@','-']\n",
    "\n",
    "def preprocess(raw):\n",
    "    line = re.sub('[%s]' % ''.join(chars), ' ', (raw))\n",
    "    words = line.split(' ')\n",
    "    words = [w.lower() for w in words]\n",
    "    words = [w for w in words if w not in stopWords]\n",
    "    words = [wnl.lemmatize(w) for w in words]\n",
    "    processed = ' '.join([porter.stem(w) for w in words])\n",
    "    \n",
    "    return processed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(29, '-'), (30, 'food'), (29, 'well'), (30, 'place'), (41, 'Great'), (34, 'best'), (31, 'it.'), (55, 'film'), (53, 'one'), (49, 'phone'), (99, 'good'), (41, 'like'), (85, 'great'), (45, 'really'), (33, 'love')]\n",
      "[(29, 'film'), (29, 'it.'), (29, 'get'), (29, 'really'), (31, 'place'), (31, 'good'), (32, 'even'), (33, 'food'), (45, 'phone'), (42, 'bad'), (47, 'would'), (38, '-'), (31, 'time'), (34, 'one'), (49, 'like')]\n"
     ]
    }
   ],
   "source": [
    "#Finding most common words in pos/neg classes\n",
    "import heapq\n",
    "\n",
    "positives = {}\n",
    "negatives = {}\n",
    "\n",
    "for i, sentence in enumerate(X_train):\n",
    "    words = sentence.split(\" \")\n",
    "    #add bigrams\n",
    "    #for i in range(len(words)-1):\n",
    "    #    words.append(words[i]+\" \"+words[i+1])\n",
    "    if Y_train[i] == 1:\n",
    "        for word in words:\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "            if word in positives:\n",
    "                positives[word] += 1  \n",
    "            else:\n",
    "                positives[word] = 1\n",
    "    else:\n",
    "        for word in words:\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "            if word in negatives:\n",
    "                negatives[word] += 1\n",
    "            else:\n",
    "                negatives[word] = 1 \n",
    "\n",
    "def maximumN(mydict, N):\n",
    "    myheap = []\n",
    "    count = 0\n",
    "    for key in mydict:\n",
    "        if count == N:\n",
    "            heapq.heappush(myheap, (mydict[key], key))\n",
    "            heapq.heappop(myheap)\n",
    "            \n",
    "        else:\n",
    "            heapq.heappush(myheap, (mydict[key], key))\n",
    "            count += 1\n",
    "            \n",
    "    print(myheap)\n",
    "\n",
    "stopWords.extend([\"I\",\"movie\",\"It\",\"The\",\"This\"])\n",
    "maximumN(positives, 15)\n",
    "maximumN(negatives, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_features(char, word, cap):\n",
    "    toReturn = []\n",
    "    if char or word or cap:\n",
    "        print(\"adding new features\")\n",
    "    if char: #compute character length feature\n",
    "        char_train = [len(x) for x in X_train]\n",
    "        char_test = [len(x) for x in X_test]\n",
    "        toReturn.append((char_train, char_test))\n",
    "    else:\n",
    "        toReturn.append(None)\n",
    "\n",
    "    if word: #compute word length feature\n",
    "        word_train = [len(x.split(\" \")) for x in X_train]\n",
    "        word_test = [len(x.split(\" \")) for x in X_test]\n",
    "        toReturn.append((word_train, word_test))\n",
    "    else:\n",
    "        toReturn.append(None)\n",
    "        \n",
    "    if cap: #compute cap count feature\n",
    "        cap_train = [sum([1 if x.isalpha() and x.isupper() else 0 for x in row]) for row in X_train]\n",
    "        cap_test = [sum([1 if x.isalpha() and x.isupper() else 0 for x in row]) for row in X_test]\n",
    "        toReturn.append((cap_train, cap_test))\n",
    "    else:\n",
    "        toReturn.append(None)\n",
    "        \n",
    "    return toReturn\n",
    "\n",
    "#print(\"Before\")\n",
    "#print(\"mean: \", np.mean(cap_train))\n",
    "#print(\"sd: \", np.std(cap_train))\n",
    "#print(\"min: \", min(cap_train))\n",
    "#print(\"max: \", max(cap_train))\n",
    "\n",
    "def warp(features, log, std, reg): #warp to sd of 1\n",
    "    toReturn = []\n",
    "    \n",
    "    for i in range(0,len(features)):\n",
    "        if features[i] is not None:\n",
    "            if std:\n",
    "                sd = np.std(features[i][0])\n",
    "                train = [x / sd for x in features[i][0]]\n",
    "                test = [x / sd for x in features[i][1]]\n",
    "            elif log:\n",
    "                train = [np.log(x) for x in features[i][0]]\n",
    "                test = [np.log(x) for x in features[i][1]]\n",
    "            else:\n",
    "                train = features[i][0]\n",
    "                test = features[i][1]\n",
    "            toReturn.append((train,test))\n",
    "        else:\n",
    "            toReturn.append(None)\n",
    "        \n",
    "    return toReturn\n",
    "\n",
    "def add_features(features, train, test):\n",
    "    for i in range(0, len(features)):\n",
    "        if features[i] is not None: \n",
    "            train = scipy.sparse.hstack((train,np.array(features[i][0])[:,None]))\n",
    "            test = scipy.sparse.hstack((test,np.array(features[i][1])[:,None]))\n",
    "    return (train,test)\n",
    "\n",
    "#print(\"After\")\n",
    "#print(\"mean: \", np.mean(X_cap_train))\n",
    "#print(\"sd: \", np.std(X_cap_train))\n",
    "#print(\"min: \", min(X_cap_train))\n",
    "#print(\"max: \", max(X_cap_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new features\n",
      "Statistics for Char Count\n",
      "mean:  64.60433333333333 64.44541666666667 65.24\n",
      "sd:    43.90844012892687 44.13218236910251 42.99595794955614\n",
      "min:   5 5 9\n",
      "max:   477 477 283\n",
      "Statistics for Word Count\n",
      "mean:  11.831666666666667 11.7875 12.008333333333333\n",
      "sd:    7.871128501612008 7.883411513002054 7.819309254801362\n",
      "min:   1 1 1\n",
      "max:   71 71 53\n",
      "Statistics for Cap Count\n",
      "mean:  2.0616666666666665 2.0820833333333333 1.98\n",
      "sd:    3.1380244138983717 3.2777551758872345 2.50058659784726\n",
      "min:   0 0 0\n",
      "max:   78 78 31\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#print(\"length of train: \", len(X_train))\n",
    "#print(\"length of test: \", len(X_test))\n",
    "\n",
    "features = new_features(True, True, True)\n",
    "title = [\"Char\", \"Word\", \"Cap\"]\n",
    "\n",
    "\n",
    "for i in range(len(features)):\n",
    "    train = features[i][0]\n",
    "    test = features[i][1]\n",
    "    both = list(train)\n",
    "    both.extend(test)\n",
    "    print(\"Statistics for \" + title[i] + \" Count\")\n",
    "    print(\"mean: \", np.mean(both), np.mean(train), np.mean(test))\n",
    "    print(\"sd:   \", np.std(both), np.std(train), np.std(test))\n",
    "    print(\"min:  \", min(both), min(train), min(test))\n",
    "    print(\"max:  \", max(both), max(train), max(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_out = []\n",
    "X_test_out = []\n",
    "\n",
    "def feature_extension(char, word, cap):\n",
    "    global X_train_out, X_test_out\n",
    "    #print(X_train_out.getnnz(1)) #nonzeros across row\n",
    "\n",
    "    #add features\n",
    "    features = new_features(char, word, cap) #char, word, cap\n",
    "    warped = warp(features, False, True, False) #log, std, reg\n",
    "    result = add_features(warped, X_train_out, X_test_out)\n",
    "    X_train_out = result[0]\n",
    "    X_test_out = result[1]\n",
    "\n",
    "    #print(X_train_out.getnnz(1)) #nonzeros across row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import metrics\n",
    "\n",
    "def count(multi, maxgram, tfidf, svd, chi, char, word, cap):\n",
    "    global X_train_out, X_test_out\n",
    "    if tfidf:\n",
    "        print(\"extracting tfidf...\")\n",
    "        counter = TfidfVectorizer(use_idf=True, preprocessor=preprocess, ngram_range=(1, maxgram))\n",
    "        X_train_out = counter.fit_transform(X_train)\n",
    "        X_test_out = counter.transform(X_test)\n",
    "    else:\n",
    "        if multi:\n",
    "            print(\"multinomial distribution\")\n",
    "        else:\n",
    "            print(\"bernoulli distribution\")\n",
    "        print(\"maxgram: \" + str(maxgram))\n",
    "        counter = CountVectorizer(preprocessor=preprocess, binary=multi, ngram_range=(1, maxgram))\n",
    "        X_train_out = counter.fit_transform(X_train)\n",
    "        X_test_out = counter.transform(X_test)\n",
    "    \n",
    "    if svd:\n",
    "        print(\"SVD dimenstionality reduction\")\n",
    "        svd = TruncatedSVD(n_components=100)\n",
    "        X_train_out = svd.fit_transform(X_train_out)\n",
    "        X_test_out = svd.transform(X_test_out)\n",
    "    \n",
    "    elif chi:\n",
    "        print(\"chi2 feature reduction\")\n",
    "        kbest = SelectKBest(chi2, k=100)\n",
    "        X_train_out = kbest.fit_transform(X_train_out, Y_train)\n",
    "        X_test_out = kbest.transform(X_test_out)\n",
    "    feature_extension(char, word, cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gridsearchSVM(train_feature, train_label, test_feature, test_label):\n",
    "    params = {\"kernel\":[ \"linear\", \"poly\", \"rbf\", \"sigmoid\"], \n",
    "           \"C\":[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "    scoring = metrics.make_scorer(metrics.accuracy_score)\n",
    "    grid = GridSearchCV(svm.SVC(random_state=0), params, cv=5,\n",
    "                        scoring=scoring)\n",
    "    grid.fit(train_feature, train_label)\n",
    "\n",
    "    print(\"best parameters: \")\n",
    "    print(grid.best_estimator_)\n",
    "    preds = grid.predict(test_feature)\n",
    "\n",
    "    print(\"Accuracy: \" + str(metrics.accuracy_score(preds, test_label)))\n",
    "    print(\"F1: \" + str(metrics.f1_score(preds, test_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def execute(bern, mult, rf, knn, dt, svm, multi, maxgram, tfidf, svd, chi, char, word, cap):\n",
    "    global X_train_out, X_test_out\n",
    "    \n",
    "    start = time.time()\n",
    "    if bern:\n",
    "        clf = BernoulliNB()\n",
    "        print(\"<Bernoulli NB>\")\n",
    "    elif mult:\n",
    "        print(\"<Multinomial NB>\")\n",
    "        clf = MultinomialNB()\n",
    "    elif rf:\n",
    "        print(\"<Random Forest>\")\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "    elif knn:\n",
    "        print(\"<KNN>\")\n",
    "        clf = KNeighborsClassifier(n_neighbors=50)\n",
    "    elif dt:\n",
    "        print(\"<Decision Tree>\")\n",
    "        clf = DecisionTreeClassifier(random_state=0)\n",
    "    else:\n",
    "        x = 1\n",
    "    \n",
    "    count(multi, maxgram, tfidf, svd, chi, char, word, cap)\n",
    "    \n",
    "    if svm:\n",
    "        print(\"<SVM>\")\n",
    "        gridsearchSVM(X_train_out, Y_train, X_test_out, Y_test)\n",
    "    else:\n",
    "        clf.fit(X_train_out, Y_train)\n",
    "        Y_pred = clf.predict(X_test_out)\n",
    "        print(\"Accuracy: \" + str(metrics.accuracy_score(Y_pred, Y_test)))\n",
    "        print(\"F1: \" + str(metrics.f1_score(Y_pred, Y_test)))  \n",
    "        \n",
    "    end = time.time()\n",
    "    print(\"run time: \" + str(end - start))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Bernoulli NB>\n",
      "bernoulli distribution\n",
      "maxgram: 1\n",
      "Accuracy: 0.808333333333\n",
      "F1: 0.801381692573\n",
      "run time: 3.7529489994\n"
     ]
    }
   ],
   "source": [
    "execute(bern=True, mult=False, rf=False, knn=False, dt=False, svm=False, \n",
    "        multi=False, maxgram=1, tfidf=False, svd=False, chi=False, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Multinomial NB>\n",
      "multinomial distribution\n",
      "maxgram: 1\n",
      "Accuracy: 0.811666666667\n",
      "F1: 0.807495741056\n",
      "run time: 0.974588871002\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=True, rf=False, knn=False, dt=False, svm=False, \n",
    "        multi=True, maxgram=1, tfidf=False, svd=False, chi=False, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KNN>\n",
      "multinomial distribution\n",
      "maxgram: 1\n",
      "Accuracy: 0.638333333333\n",
      "F1: 0.479616306954\n",
      "run time: 1.01091599464\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=False, rf=False, knn=True, dt=False, svm=False, \n",
    "        multi=True, maxgram=1, tfidf=False, svd=False, chi=False, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Decision Tree>\n",
      "multinomial distribution\n",
      "maxgram: 1\n",
      "Accuracy: 0.78\n",
      "F1: 0.774744027304\n",
      "run time: 1.1004178524\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=False, rf=False, knn=False, dt=True, svm=False, \n",
    "        multi=True, maxgram=1, tfidf=False, svd=False, chi=False, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Random Forest>\n",
      "multinomial distribution\n",
      "maxgram: 1\n",
      "Accuracy: 0.8\n",
      "F1: 0.78102189781\n",
      "run time: 2.43800497055\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=False, rf=True, knn=False, dt=False, svm=False, \n",
    "        multi=True, maxgram=1, tfidf=False, svd=False, chi=False, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomial distribution\n",
      "maxgram: 1\n",
      "<SVM>\n",
      "best parameters: \n",
      "SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='sigmoid',\n",
      "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy: 0.818333333333\n",
      "F1: 0.805008944544\n",
      "run time: 67.9869029522\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=False, rf=False, knn=False, dt=False, svm=True, \n",
    "        multi=True, maxgram=1, tfidf=False, svd=False, chi=False, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Bernoulli NB>\n",
      "bernoulli distribution\n",
      "maxgram: 2\n",
      "Accuracy: 0.816666666667\n",
      "F1: 0.804270462633\n",
      "run time: 1.12560200691\n"
     ]
    }
   ],
   "source": [
    "execute(bern=True, mult=False, rf=False, knn=False, dt=False, svm=False, \n",
    "        multi=False, maxgram=2, tfidf=False, svd=False, chi=False, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Multinomial NB>\n",
      "multinomial distribution\n",
      "maxgram: 2\n",
      "Accuracy: 0.818333333333\n",
      "F1: 0.814310051107\n",
      "run time: 1.24065494537\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=True, rf=False, knn=False, dt=False, svm=False, \n",
    "        multi=True, maxgram=2, tfidf=False, svd=False, chi=False, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Decision Tree>\n",
      "multinomial distribution\n",
      "maxgram: 2\n",
      "Accuracy: 0.775\n",
      "F1: 0.761061946903\n",
      "run time: 1.64271783829\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=False, rf=False, knn=False, dt=True, svm=False, \n",
    "        multi=True, maxgram=2, tfidf=False, svd=False, chi=False, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Random Forest>\n",
      "multinomial distribution\n",
      "maxgram: 2\n",
      "Accuracy: 0.791666666667\n",
      "F1: 0.760076775432\n",
      "run time: 3.74967217445\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=False, rf=True, knn=False, dt=False, svm=False, \n",
    "        multi=True, maxgram=2, tfidf=False, svd=False, chi=False, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomial distribution\n",
      "maxgram: 2\n",
      "<SVM>\n",
      "best parameters: \n",
      "SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy: 0.816666666667\n",
      "F1: 0.798534798535\n",
      "run time: 89.0351891518\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=False, rf=False, knn=False, dt=False, svm=True, \n",
    "        multi=True, maxgram=2, tfidf=False, svd=False, chi=False, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Multinomial NB>\n",
      "extracting tfidf...\n",
      "Accuracy: 0.821666666667\n",
      "F1: 0.818336162988\n",
      "run time: 1.24855899811\n"
     ]
    }
   ],
   "source": [
    "#Good\n",
    "execute(bern=False, mult=True, rf=False, knn=False, dt=False, svm=False, \n",
    "        multi=True, maxgram=2, tfidf=True, svd=False, chi=False, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Random Forest>\n",
      "extracting tfidf...\n",
      "Accuracy: 0.808333333333\n",
      "F1: 0.795008912656\n",
      "run time: 3.53962922096\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=False, rf=True, knn=False, dt=False, svm=False, \n",
    "        multi=True, maxgram=2, tfidf=True, svd=False, chi=False, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting tfidf...\n",
      "<SVM>\n",
      "best parameters: \n",
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy: 0.838333333333\n",
      "F1: 0.834188034188\n",
      "run time: 91.3393120766\n"
     ]
    }
   ],
   "source": [
    "#good\n",
    "execute(bern=False, mult=False, rf=False, knn=False, dt=False, svm=True, \n",
    "        multi=True, maxgram=2, tfidf=True, svd=False, chi=False, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Multinomial NB>\n",
      "extracting tfidf...\n",
      "adding new features\n",
      "Accuracy: 0.813333333333\n",
      "F1: 0.804195804196\n",
      "run time: 1.26393198967\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=True, rf=False, knn=False, dt=False, svm=False, \n",
    "        multi=True, maxgram=2, tfidf=True, svd=False, chi=False, \n",
    "        char=True, word=True, cap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Multinomial NB>\n",
      "multinomial distribution\n",
      "maxgram: 2\n",
      "adding new features\n",
      "Accuracy: 0.813333333333\n",
      "F1: 0.806228373702\n",
      "run time: 1.14739179611\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=True, rf=False, knn=False, dt=False, svm=False, \n",
    "        multi=True, maxgram=2, tfidf=False, svd=False, chi=False, \n",
    "        char=True, word=True, cap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Random Forest>\n",
      "multinomial distribution\n",
      "maxgram: 2\n",
      "adding new features\n",
      "Accuracy: 0.783333333333\n",
      "F1: 0.750957854406\n",
      "run time: 3.56088113785\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=False, rf=True, knn=False, dt=False, svm=False, \n",
    "        multi=True, maxgram=2, tfidf=False, svd=False, chi=False, \n",
    "        char=True, word=True, cap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Random Forest>\n",
      "extracting tfidf...\n",
      "adding new features\n",
      "Accuracy: 0.795\n",
      "F1: 0.779964221825\n",
      "run time: 3.24257588387\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=False, rf=True, knn=False, dt=False, svm=False, \n",
    "        multi=True, maxgram=2, tfidf=True, svd=False, chi=False, \n",
    "        char=True, word=True, cap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomial distribution\n",
      "maxgram: 2\n",
      "adding new features\n",
      "<SVM>\n",
      "best parameters: \n",
      "SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy: 0.82\n",
      "F1: 0.801470588235\n",
      "run time: 108.476480007\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=False, rf=False, knn=False, dt=False, svm=True, \n",
    "        multi=True, maxgram=2, tfidf=False, svd=False, chi=False, \n",
    "        char=True, word=True, cap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting tfidf...\n",
      "adding new features\n",
      "<SVM>\n",
      "best parameters: \n",
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy: 0.841666666667\n",
      "F1: 0.834782608696\n",
      "run time: 118.193981886\n"
     ]
    }
   ],
   "source": [
    "#good\n",
    "execute(bern=False, mult=False, rf=False, knn=False, dt=False, svm=True, \n",
    "        multi=True, maxgram=2, tfidf=True, svd=False, chi=False, \n",
    "        char=True, word=True, cap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Random Forest>\n",
      "extracting tfidf...\n",
      "SVD dimenstionality reduction\n",
      "Accuracy: 0.741666666667\n",
      "F1: 0.73321858864\n",
      "run time: 3.79168009758\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=False, rf=True, knn=False, dt=False, svm=False, \n",
    "        multi=True, maxgram=2, tfidf=True, svd=True, chi=False, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting tfidf...\n",
      "SVD dimenstionality reduction\n",
      "<SVM>\n",
      "best parameters: \n",
      "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy: 0.748333333333\n",
      "F1: 0.735551663748\n",
      "run time: 149.725368977\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=False, rf=False, knn=False, dt=False, svm=True, \n",
    "        multi=True, maxgram=2, tfidf=True, svd=True, chi=False, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Random Forest>\n",
      "extracting tfidf...\n",
      "chi2 feature reduction\n",
      "Accuracy: 0.756666666667\n",
      "F1: 0.708\n",
      "run time: 1.47477698326\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=False, rf=True, knn=False, dt=False, svm=False, \n",
    "        multi=True, maxgram=2, tfidf=True, svd=False, chi=True, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Random Forest>\n",
      "extracting tfidf...\n",
      "chi2 feature reduction\n",
      "adding new features\n",
      "Accuracy: 0.748333333333\n",
      "F1: 0.746218487395\n",
      "run time: 2.00016522408\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=False, rf=True, knn=False, dt=False, svm=False, \n",
    "        multi=True, maxgram=2, tfidf=True, svd=False, chi=True, \n",
    "        char=True, word=True, cap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting tfidf...\n",
      "chi2 feature reduction\n",
      "<SVM>\n",
      "best parameters: \n",
      "SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy: 0.768333333333\n",
      "F1: 0.724752475248\n",
      "run time: 19.3123428822\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=False, rf=False, knn=False, dt=False, svm=True, \n",
    "        multi=True, maxgram=2, tfidf=True, svd=False, chi=True, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting tfidf...\n",
      "chi2 feature reduction\n",
      "adding new features\n",
      "<SVM>\n",
      "best parameters: \n",
      "SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy: 0.768333333333\n",
      "F1: 0.724752475248\n",
      "run time: 624.191854954\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=False, rf=False, knn=False, dt=False, svm=True, \n",
    "        multi=True, maxgram=2, tfidf=True, svd=False, chi=True, \n",
    "        char=True, word=True, cap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Multinomial NB>\n",
      "extracting tfidf...\n",
      "chi2 feature reduction\n",
      "Accuracy: 0.77\n",
      "F1: 0.722891566265\n",
      "run time: 0.994335889816\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=True, rf=False, knn=False, dt=False, svm=False, \n",
    "        multi=True, maxgram=2, tfidf=True, svd=False, chi=True, \n",
    "        char=False, word=False, cap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Multinomial NB>\n",
      "extracting tfidf...\n",
      "chi2 feature reduction\n",
      "adding new features\n",
      "Accuracy: 0.771666666667\n",
      "F1: 0.728712871287\n",
      "run time: 1.13313698769\n"
     ]
    }
   ],
   "source": [
    "execute(bern=False, mult=True, rf=False, knn=False, dt=False, svm=False, \n",
    "        multi=True, maxgram=2, tfidf=True, svd=False, chi=True, \n",
    "        char=True, word=True, cap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X_train + X_test\n",
    "Y = Y_train + Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "def validate_model(clf, features, label, char_feat):\n",
    "    accuracy_result = []\n",
    "    f1_result = []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    i = 0\n",
    "    for train_ind, test_ind in kf.split(features):\n",
    "        train_feat = features[train_ind]\n",
    "        test_feat = features[test_ind]\n",
    "        \n",
    "        counter = TfidfVectorizer(use_idf=True, preprocessor=preprocess, ngram_range=(1, 2))\n",
    "        train_feat_out = counter.fit_transform(train_feat)\n",
    "        test_feat_out = counter.transform(test_feat)\n",
    "        \n",
    "        if char_feat:\n",
    "            char_train = [len(x) for x in train_feat]\n",
    "            char_test = [len(x) for x in test_feat]\n",
    "            char_sd = np.std(char_train)\n",
    "            char_train = [x / char_sd for x in char_train]\n",
    "            char_test = [x / char_sd for x in char_test]\n",
    "            train_feat_out = scipy.sparse.hstack((train_feat_out,np.array(char_train)[:,None]))\n",
    "            test_feat_out = scipy.sparse.hstack((test_feat_out,np.array(char_test)[:,None]))\n",
    "            \n",
    "            word_train = [len(x.split(\" \")) for x in train_feat]\n",
    "            word_test = [len(x.split(\" \")) for x in test_feat]\n",
    "            word_sd = np.std(word_train)\n",
    "            word_train = [x / word_sd for x in word_train]\n",
    "            word_test = [x / word_sd for x in word_test]\n",
    "            train_feat_out = scipy.sparse.hstack((train_feat_out,np.array(word_train)[:,None]))\n",
    "            test_feat_out = scipy.sparse.hstack((test_feat_out,np.array(word_test)[:,None]))\n",
    "            \n",
    "            cap_train = [sum([1 if x.isalpha() and x.isupper() else 0 for x in row]) for row in train_feat]\n",
    "            cap_test = [sum([1 if x.isalpha() and x.isupper() else 0 for x in row]) for row in test_feat]\n",
    "            cap_sd = np.std(cap_train)\n",
    "            cap_train = [x / cap_sd for x in cap_train]\n",
    "            cap_test = [x / cap_sd for x in cap_test]\n",
    "            train_feat_out = scipy.sparse.hstack((train_feat_out,np.array(cap_train)[:,None]))\n",
    "            test_feat_out = scipy.sparse.hstack((test_feat_out,np.array(cap_test)[:,None]))\n",
    "            \n",
    "            \n",
    "            \n",
    "        i = i + 1\n",
    "        print ('Fold {}'.format(i))\n",
    "        clf.fit(train_feat_out, label[train_ind])\n",
    "        Y_pred = clf.predict(test_feat_out)\n",
    "        accuracy = metrics.accuracy_score(Y_pred, label[test_ind])\n",
    "        f1score = metrics.f1_score(Y_pred, label[test_ind])\n",
    "        accuracy_result.append(accuracy)\n",
    "        f1_result.append(f1score)\n",
    "\n",
    "        print ('Accuracy:{}'.format(accuracy))\n",
    "        print ('F1 Score: {}'.format(f1score))\n",
    "\n",
    "    print ('Overall Accuracy: {}'.format(np.mean(accuracy_result)))\n",
    "    print ('Overall F1 Score: {}'.format(np.mean(f1_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Accuracy:0.82\n",
      "F1 Score: 0.823529411765\n",
      "Fold 2\n",
      "Accuracy:0.82\n",
      "F1 Score: 0.828025477707\n",
      "Fold 3\n",
      "Accuracy:0.803333333333\n",
      "F1 Score: 0.797250859107\n",
      "Fold 4\n",
      "Accuracy:0.81\n",
      "F1 Score: 0.820189274448\n",
      "Fold 5\n",
      "Accuracy:0.84\n",
      "F1 Score: 0.843648208469\n",
      "Overall Accuracy: 0.818666666667\n",
      "Overall F1 Score: 0.822528646299\n"
     ]
    }
   ],
   "source": [
    "X = X_train + X_test\n",
    "Y = Y_train + Y_test\n",
    "validate_model(MultinomialNB(), np.asarray(X), np.asarray(Y),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Accuracy:0.806666666667\n",
      "F1 Score: 0.80204778157\n",
      "Fold 2\n",
      "Accuracy:0.835\n",
      "F1 Score: 0.8416\n",
      "Fold 3\n",
      "Accuracy:0.815\n",
      "F1 Score: 0.806956521739\n",
      "Fold 4\n",
      "Accuracy:0.803333333333\n",
      "F1 Score: 0.810897435897\n",
      "Fold 5\n",
      "Accuracy:0.828333333333\n",
      "F1 Score: 0.831423895254\n",
      "Overall Accuracy: 0.817666666667\n",
      "Overall F1 Score: 0.818585126892\n"
     ]
    }
   ],
   "source": [
    "validate_model(svm.SVC(random_state=0, C=1, kernel=\"linear\"), np.asarray(X), np.asarray(Y),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Accuracy:0.8\n",
      "F1 Score: 0.797297297297\n",
      "Fold 2\n",
      "Accuracy:0.833333333333\n",
      "F1 Score: 0.839743589744\n",
      "Fold 3\n",
      "Accuracy:0.811666666667\n",
      "F1 Score: 0.804835924007\n",
      "Fold 4\n",
      "Accuracy:0.803333333333\n",
      "F1 Score: 0.810897435897\n",
      "Fold 5\n",
      "Accuracy:0.833333333333\n",
      "F1 Score: 0.837133550489\n",
      "Overall Accuracy: 0.816333333333\n",
      "Overall F1 Score: 0.817981559487\n"
     ]
    }
   ],
   "source": [
    "validate_model(svm.SVC(random_state=0, C=1, kernel=\"linear\"), np.asarray(X), np.asarray(Y),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_informative(clf, train_feat, train_label, char_feat):\n",
    "    accuracy_result = []\n",
    "    f1_result = []\n",
    "        \n",
    "    counter = TfidfVectorizer(use_idf=True, preprocessor=preprocess, ngram_range=(1, 2))\n",
    "    train_feat_out = counter.fit_transform(train_feat)\n",
    "    feat_name = counter.get_feature_names()\n",
    "        \n",
    "    if char_feat:\n",
    "        char_train = [len(x) for x in train_feat]\n",
    "        char_sd = np.std(char_train)\n",
    "        char_train = [x / char_sd for x in char_train]\n",
    "        train_feat_out = scipy.sparse.hstack((train_feat_out,np.array(char_train)[:,None]))\n",
    "\n",
    "        word_train = [len(x.split(\" \")) for x in train_feat]\n",
    "        word_sd = np.std(word_train)\n",
    "        word_train = [x / word_sd for x in word_train]\n",
    "        train_feat_out = scipy.sparse.hstack((train_feat_out,np.array(word_train)[:,None]))\n",
    "\n",
    "        cap_train = [sum([1 if x.isalpha() and x.isupper() else 0 for x in row]) for row in train_feat]\n",
    "        cap_sd = np.std(cap_train)\n",
    "        cap_train = [x / cap_sd for x in cap_train]\n",
    "        train_feat_out = scipy.sparse.hstack((train_feat_out,np.array(cap_train)[:,None]))\n",
    "        \n",
    "        feat_name = feat_name + [\"Char Count\", \"Word Count\", \"Uppercase Count\"]\n",
    "            \n",
    "            \n",
    "    clf.fit(train_feat_out, train_label)\n",
    "    print(\"Positive Class :\")\n",
    "    informative = np.argsort(clf.coef_[0])[-10:]\n",
    "    for i in informative:\n",
    "        print(feat_name[i])\n",
    "    \n",
    "    print(\"\\nNegative Class :\")\n",
    "    informative = np.argsort(clf.coef_[0])[0:10]\n",
    "    for i in informative:\n",
    "        print(feat_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Class :\n",
      "well\n",
      "excel\n",
      "food\n",
      "movi\n",
      "film\n",
      "work\n",
      "phone\n",
      "love\n",
      "good\n",
      "great\n",
      "\n",
      "Negative Class :\n",
      "00\n",
      "miss numer\n",
      "miss entir\n",
      "misplac\n",
      "mislead\n",
      "mishima extrem\n",
      "miser hollow\n",
      "miser\n",
      "mirrormask last\n",
      "mirrormask\n"
     ]
    }
   ],
   "source": [
    "most_informative(MultinomialNB(), X_train, Y_train, char_feat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add more data!\n",
    "# read in dataset\n",
    "\n",
    "def extend():\n",
    "    global X_train, Y_train\n",
    "    with open(\"Sentiment Analysis Dataset.csv\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    newX = []\n",
    "    newY = []\n",
    "\n",
    "    for line in lines[1:]:\n",
    "        newY.append(int(line.split(',')[1]))\n",
    "        raw = ' '.join(line.rsplit()[1:])\n",
    "        newX.append(raw)\n",
    "\n",
    "    X_train = X_train + newX\n",
    "    Y_train = Y_train + newY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
